 <!doctype html>
 <html lang="en">

 <head>
     <!-- Global site tag (gtag.js) - Google Analytics -->
     <script async src="https://www.googletagmanager.com/gtag/js?id=UA-145676546-1"></script>
     <script>
         window.dataLayer = window.dataLayer || [];

         function gtag() {
             dataLayer.push(arguments);
         }
         gtag('js', new Date());

         gtag('config', 'UA-145676546-1');
     </script>


     <meta charset="utf-8">
     <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
     <meta name="description" content="">

     <title>XaiPient</title>

     <!-- Bootstrap core CSS -->
     <link href="css/bootstrap.min.css" rel="stylesheet" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">


     <!-- Custom styles for this template -->
     <link href="/css/style.css" rel="stylesheet">
 </head>

 <body>
     <!--Menu-->

     <nav class="navbar sticky-top bio pr-5 pl-5 no-gutters">
         <div class="col-12 col-md-4 text-md-left text-center">
             <a class="menu" href="index.html">Ricardo Saavedra</a>
         </div>

         <div class="col-12 col-md-4 text-md-center text-center">
             <a href="sol.html" class="menu">Prev</a>
             <span>/</span>
             <a href="NAM.html" class="menu">Next</a>
         </div>

         <!--Work/Info-->
         <div class="col-12 col-md-4 text-md-right text-center">
             <a href="index.html" class="menu-active menu">Work</a>
             <span>/</span>
             <a href="info.html" class="menu">Info</a>
         </div>
     </nav>

     <!--New top-->
     <div class="container-fluid full">
         <div class="row no-gutters">
             <div class="col-12 top-proj-container img-gradient">
                 <h1 class="proj-title2-new">A new approach to machine learning</h1>
                 <img src="/img/xai/cover.png" class="img-fluid gradient-img" alt="Responsive image">
             </div>
         </div>


     </div>

     <div class="container-fluid ">
         <div class="row bg-true-black mb-5 pb-5">
             <div class="col-12 pl-5 pr-5">
                 <h2 class="title3-new text-white">XaiPient</h2>
                 <p class="text1-new text-white">XaiPient’s mission is to make AI transparent and easy to understand by offering ways to inspect the “whys” behind AI systems’ decisions. A big problem of machine models today is that they are good at predicting certain outcomes with a high degree of accuracy, but fail at explaining how they arrive at those outcomes (a problem known as black-box). As more aspects of the world are automated by AI, knowing how AI models operate is crucial to prevent accidents andbiases when deploying these systems in the wild.<p>
             </div>


         </div>

         <div class="row">
             <div class="col-6 pl-5 pr-5">
                 <h2 class="title3-new">Approach</h2>
                 <p class="text2-new">I joined the founding team for 4 months to help them turn years of technical research into a tangible product. I helped the product team make sense of the fast-growing space of interpretability (XAI) by mapping out different competitors, investigating potential new technologies, and identifying relevant issues regarding the design of explanations in different applications, for different users, and domains. I also produced prototypes after gathering insights from users and validating hypotheses about the value proposition of certain features and aspects of the product.</p>

                 <h2 class="title3-new">Problem</h2>
                 <p class="text2-new">The biggest design challenge I faced was: how do we build human-friendly explanations that let users understand what influenced a machine decision without relying on technical concepts and complex data visualizations? Additionally, how do we communicate decisions effectively for different levels of literacy and use cases? Since machine learning is based on probability, how do we communicate uncertainty without influencing user behavior? And finally, how do we prevent explanations from being misunderstood, creating serious risks and consequences for those affected by these very systems?</p>

                 <h2 class="title3-new">Acheivements</h2>
                 <p class="text2-new">After validating different hypotheses and having gained a wider understanding of different use cases, I shifted my focus towards ideating on solutions. I prioritized thinking in components and design patterns. I then worked on low fidelity prototypes to validate the concepts that had more traction with the team to test them in the contexts they were designed for before moving on to designing the grid, color system, data-centric elements and typography. I handed off the project before conducting user tests and iterating on concepts based on user feedback.</p>
             </div>

             <div class="col-3 pr-5">
                 <h2 class="title3-new">Services</h2>
                 <p class="text2-new">Most applications of</p>

                 <h2 class="title3-new">Capabilities</h2>
                 <p class="text2-new">Most applications of</p>

                 <h2 class="title3-new">Approac</h2>
                 <p class="text2-new">Most applications of</p>


             </div>

             <div class="col-3 pr-5">
                 <h2 class="title3-new">Location</h2>
                 <p class="text2-new">Most applications of</p>

                 <h2 class="title3-new">Duration</h2>
                 <p class="text2-new">Most applications of</p>

                 <h2 class="title3-new">Methods</h2>
                 <p class="text2-new">Most applications of</p>

             </div>

         </div>
     </div>


     <div class="container-fluid full">
         <div class="row no-gutters">
             <div class="col-12 pl-5 pr-5 border-top-bottom section-division ">
                 <h2 class="title1-new pr-5"><span>01</span>User Research</h2>
             </div>
         </div>

         <div class="row no-gutters justify-content-start">
             <div class="col-6 pl-5 pr-5">
                 <h2 class="title3-new ">The problem</h2>
                 <p class="text2-new"> A big problem of machine models today is that they are good at predicting certain outcomes with a high degree of accuracy, but fail at explaining how they arrive at those outcomes (a problem known as black-box). As more aspects of the world are automated by AI, knowing how AI models operate is crucial to prevent accidents andbiases when deploying these systems in the wild.</p>
             </div>
         </div>

         <div class="row no-gutters justify-content-center">
             <div class="col-10 mb-5 pl-5 pr-5"><img src="/img/xai/xai%20intro%20graph.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>
         </div>

         <div class="row no-gutters justify-content-start">
             <div class="col-6 pl-5 pr-5">
                 <h2 class="title3-new ">Mapping out the interpretability space</h2>
                 <p class="text2-new">Since AI effects so many aspects of society today, I knew it was essential to include different voices and perspectives beyond the ones found in the team. Thinking about the problem through a computational and information design lens has only solved part of the puzzle. It was necessary to go beyond initial assumptions if we were to design solutions that addressed the problem holistically. To extend the scope of the research, I interviewed cognitive scientists, statisticians, HCI researchers, and scholars involved in algorithmic fairness to uncover insights about the challenges and limitations of "explaining AI”, and its implications for society at large.</p>
             </div>

             <div class="col-6 mb-5 mt-5"><img src="/img/xai/quotes.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>

             <div class="col-12 pl-5 pr-5 mb-5">
                 <h2 class="title2-new ">Interpretable machine learning is a complex problem that involves many disciplines, industries, and stakeholders</h2>
             </div>


             <div class="col-12 mb-5 pl-5 pr-5 align-self-end"><img src="/img/xai/personas-simple.png" class="img-fluid figure-img img-proj-new" alt="Responsive image">
             </div>
         </div>

         <div class="row no-gutters mt-5 mb-5">
             <div class="col-6 pl-5 pr-4">
                 <p class="title2-new">Experts interview and literature review</p>
             </div>
             <div class="col-6 pl-4 pr-5">
                 <p class="text2-new">Since AI effects so many aspects of society today, I knew it was essential to include different voices and perspectives beyond the ones found in the team. Thinking about the problem through a computational and information design lens has only solved part of the puzzle. It was necessary to go beyond initial assumptions if we were to design solutions that addressed the problem holistically. To extend the scope of the research, I interviewed cognitive scientists, statisticians, HCI researchers, and scholars involved in algorithmic fairness to uncover insights about the challenges and limitations of "explaining AI”, and its implications for society at large.</p>
             </div>
         </div>

         <div class="row no-gutters">
             <div class="col-12 mb-5">
                 <img src="/img/xai/papers%20I%20read.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>

         </div>

         <div class="row no-gutters ">
             <div class="col-12 pl-5 pr-5 mb-4">
                 <p class="text1-new">Key insights</p>
                 <p class="title2-new">Explainable AI as of today has a strong focus on mathematics and computer science to interpret “black box” models. While these are significant contributions, they tend to neglect the human side of the explanations and whether they are usable and practical in real-world situations.</p>
             </div>
             <div class="col-6 pl-5 pr-4">
                 <p class="text2-new">Explainable AI as of today has a strong focus on mathematics and computer science to interpret “black box” models. While these are significant contributions, they tend to neglect the human side of the explanations and whether they are usable and practical in real-world situations. Often, the research does not appear to be strongly informed by cognitive psychology in terms of how humans can interpret the explanations and does not deploy or evaluate the explanations in interactive applications with real users. This is echoed by Shneiderman et al., who discussed the need for interfaces that allow users “to better understand underlying computational processes” and give users “the potential to better control their (the algorithms’) actions” as one of the grand challenges for HCI researchers.</p>
             </div>
         </div>
         <div class="row no-gutters mt-5 mb-5 justify-content-center">
             <div class="col-10 pl-4 pr-5">
                 <img src="/img/xai/method+human.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>
         </div>

         <div class="row no-gutters mt-5">
             <div class="col-6 pr-4 pl-5 mb-5">
                 <h2 class="title2-new ">Use cases: where is interpretable machine learning the most valuable?</h2>
                 <p class="text2-new">The need for interpretability stems from incompleteness in the problem formalization, creating a fundamental barrier to optimization and evaluation.</p>
             </div>

             <div class="col-6 pl-4 pr-4">
                 <ol class="list-new">
                     <li><span>Scientific Understanding</span> <br> The human’s goal is to gain knowledge. We do not have a complete way of stating what knowledge is; thus the best we can do is ask for explanations we can convert into knowledge.</li>
                     <li><span>Ethics</span> <br> The user may want to guard against certain kinds of discrimination, and their notion of fairness may be too abstract to be completely encoded into the system (e.g., one might desire a ‘fair’ classifier for loan approval). Even if we can encode protections for specific protected classes into the system, there might be biases that we did not consider a priori (e.g., one may not build gender-biased word embeddings on purpose, but it was a pattern in data that became apparent only after the fact).</li>
                     <li><span>Safety</span> <br> For complex tasks, the end-to-end system is almost never completely testable; one cannot create a complete list of scenarios in which the system may fail. Enumerating all possible outputs given all possible inputs be computationally or logistically infeasible, and we may be unable to flag all undesirable outputs.</li>
                     <li><span>Mismatched objectives</span> <br>The agent’s algorithm may be optimizing an incomplete objective— that is, a proxy function for the ultimate goal. For example, a clinical system may be optimized for cholesterol control, without considering the likelihood of adherence; an automotive engineer may be interested in engine data not to make predictions about engine failures but to more broadly build a better car.</li>
                 </ol>
             </div>
         </div>

         <div class="row no-gutters mt-5">
             <div class="col-6 pr-4 pl-5 mb-5">
                 <h2 class="title2-new ">Explainability and the task being performed</h2>
                 <p class="text2-new">I complied a list of questions to help the design and development team to think about the different solutions we were coming up with as we engaged in various brainstorming exercises. The points bellow is an excerpt of a larger list with key aspects that help to frame the kinds of solutions being developed. Each idea suggested was validated and rated according to these main categories. </p>
             </div>

             <div class="col-6 pl-4 pr-4">
                 <ol class="list-new">
                     <li><span>Time Constraints</span> <br> How long can the user afford to spend to understand the explanation? A decision that needs to be made at the bedside or during the operation of a plant must be understood quickly, while in scientific or anti-discrimination applications, the end-user may be willing to spend hours trying to fully understand an explanation.</li>
                     <li><span>Global vs. Local </span> <br> Global interpretability implies knowing what patterns are present in general (such as key features governing galaxy formation), while local interpretability implies knowing the reasons for a specific decision (such as why a particular loan application was rejected). The former may be important for when scientific understanding or bias detection is the goal; the latter when one needs a justification for a specific decision.</li>
                     <li><span>User Expertise </span> <br> For example, a clinician may have a notion that autism and ADHD are both developmental diseases. The nature of the user’s expertise will also influence what level of sophistication they expect in their explanations. For example, domain experts may expect or prefer a somewhat larger and sophisticated model—which confirms facts they know—over a smaller, more opaque one. More broadly, decision-makers, scientists, compliance and safety engineers, data scientists, and machine learning researchers all come with different background knowledge and communication styles.</li>
                     <li><span>Mismatched objectives</span> <br>The agent’s algorithm may be optimizing an incomplete objective— that is, a proxy function for the ultimate goal. For example, a clinical system may be optimized for cholesterol control, without considering the likelihood of adherence; an automotive engineer may be interested in engine data not to make predictions about engine failures but to more broadly build a better car.</li>
                 </ol>
             </div>
         </div>

         <div class="row no-gutters mt-5 justify-content-start">
             <div class="col-6 pr-4 pl-5">
                 <h2 class="title3-new">Understanding user needs</h2>
                 <div class="col-10">

                 </div>
                 <p class="text2-new">In addition to the use cases, and task taxonomy, I also developed personas based on the user interviews I conducted with users and machine learning experts. I categorized users in three distinct categories: auditors, underwriter, and business users. I also crafted product stories and specific use cases related to these users to bring the design and development team on board with how the technology might be used in the end, and what are the real stories and questions each user is trying to answer. I mapped the requirements for the different components; including tone of voice, depth of explanation, level of technical literacy, and UX requirements.</p>
             </div>
         </div>
         <div class="row no-gutters justify-content-center">
             <div class="col-10 pl-4 pr-4">
                 <img src="/img/xai/personas-full.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>
         </div>
     </div>

     <div class="row no-gutters mt-5">
         <div class="col-6 pl-5 mb-5">
             <h3 class="title3-new">Ideation and brainstorm sessions</h3>
             <p class=text2-new>The research materials I developed helped to bring together in how we thought about solutions, and the big challenges we needed to overcome for each user. I organized working sessions between designers and developers to ideate on how the different solutions could look like. Some of the concepts we came up with were extremely dense, featuring heat maps and other visualization elements, and focused on facilitating the process of model debugging for data scientists. Others were much simpler and were based on counterfactual explanations ("why a certain prediction was made instead of another one"). Some of the most exciting ideas were based on logic programming and natural language processing and explored conversational AI paradigms.</p>
         </div>
         <div class="col-12">
             <img src="/img/xai/sketches.png" class="img-fluid img-proj-new" alt="Responsive image">
             <img src="/img/xai/prototype-2.png" class="img-fluid img-proj-new" alt="Responsive image">
         </div>

     </div>

     <div class="row no-gutters mt-5">
         <div class="col-6 pl-5 pr-4 mb-5">
             <h3 class="title3-new">Narrowing down the scope through low-fidelity prototypes and user interviews</h3>
             <p class=text2-new>Following brainstorming and ideation, I produced mid-fidelity prototypes and screens to gather insights from real users. Before advocating towards one idea over the other, I wanted to validate concepts with real users to undersant how valuable they saw those ideas. This allowed the team to narrow down the scope considerably, and to focus on features and aspects of the experience that were the most important to stakeholders and users.</p>

         </div>

         <div class="col-6 pl-4 pr-5 mb-5">

         </div>
     </div>

     <div class="row no-gutters bg-purple-light justify-content-start pt-5">
         <div class="col-6 pl-5 pr-5">
             <h2 class="title2-new text-white">Defining the product principles</h2>
             <p class="text2-new text-white">The research I conducted also helped to clarify the design of the system itself. Major questions of how user flows on the platform hadn't been defined. The documents I produced and the workshops I conducted helped the founders to evaluate different flows eventually settling on the one that was more aligned with real user needs.</p>
         </div>


         <div class="col-12 pl-5 pr-5">
             <img src="/img/xai/prediction-flow.png" class="img-fluid img-proj-new" alt="Responsive image">
         </div>
     </div>

     <div class="row no-gutters mt-5">
         <div class="col-12 pl-5 mb-5">
             <h3 class="title2-new text-center">System architecture</h3>
         </div>
         <div class="col-12 pl-5 pr-5 mb-5">
             <img src="/img/xai/explanation.png" class="img-fluid img-proj-new" alt="Responsive image">
         </div>
     </div>


     <div class="container-fluid full">
         <div class="row no-gutters">
             <div class="col-12 pl-5 pr-5 border-top-bottom section-division ">
                 <h2 class="title1-new pr-5"><span>03</span>Final Designs</h2>
             </div>
         </div>

         <div class="row no-gutters ">
             <div class="col-12 pl-5 pr-5 mb-4">
                 <p class="text1-new">Pattern library of explainability components</p>
                 <p class="title2-new">The final strategy was to design explainabilty components that tailored to different users, a strategy similar to tableau, a very popular data visualization software. Thinking in components was the best strategy.</p>
             </div>
             <div class="col-6 pl-5 pr-4">
                 <p class="text2-new">I prioritized thinking in components and design patterns rather than worrying about flows and secondary navigation aspects of the application. This decision enabled the team to make progress on the most important questions before the product strategy was fully defined. The goal was to design a highly customizable pattern library comprised of different explanation methods that could be included in different applications while maintaining a high level of consistency between elements.</p>
             </div>
         </div>



         <div class="row no-gutters">
             <div class="col-12 img-caption">
                 <img src="/img/xai/prototype1.png" class="img-fluid img-caption-onTop" alt="Responsive image">
                 <figcaption class="figure-caption-top text-center">A caption for the above image.</figcaption>
             </div>


         </div>

         <div class="row no-gutters">
             <div class="col-6 pl-5 pr-4">
                 <h3 class="title3-new">Collecting a diverse set of references</h3>
                 <p class=text2-new>I then worked on low fidelity prototypes to validate the concepts that had more traction with the team to test them in the contexts they were designed for before moving on to designing the grid, color system, data-centric elements and typography. I handed off the project before conducting user tests and iterating on concepts based on user feedback.</p>

             </div>


         </div>
         <div class="row no-gutters">
             <div class="col-12 img-caption">
                 <img src="/img/xai/references.png" class="img-fluid img-caption-onTop" alt="Responsive image">
                 <figcaption class="figure-caption-top text-center">A small compilation of the data visualization components I collected over the period</figcaption>
             </div>
         </div>

         <div class="row no-gutters bg-purple-light justify-content-start">
             <div class="row no-gutters justify-content-center">
                 <div class="col-12">
                     <img src="/img/xai/applicatino%20mock.png" class="img-fluid img-proj-new" alt="Responsive image">
                 </div>
             </div>
         </div>



         <div class="row no-gutters bg-purple-light justify-content-start pt-5 pb-5">
             <div class="col-6 pl-5 pr-5 mb-4">
                 <h2 class="title4-new text-white">Location</h2>
                 <h2 class="title2-new text-white">Location</h2>
                 <p class="text2-new text-white">Most applications of AI today are based on using data to train a machine learning model to predict an outcome. </p>
             </div>


             <div class="col-12 pl-5 pr-5 mb-5">
                 <img src="/img/xai/app-prediction-1.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>

         </div>

         <div class="row no-gutters bg-purple-light-alt justify-content-start pt-5 pb-5">
             <div class="col-6 pl-5 pr-5 mb-4">
                 <h2 class="title4-new text-white">Location</h2>
                 <h2 class="title2-new text-white">Location</h2>
                 <p class="text2-new text-white">Most applications of AI today are based on using data to train a machine learning model to pr</p>
             </div>


             <div class="col-12 pl-5 pr-5 mb-5">
                 <img src="/img/xai/app-prediction.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>
         </div>

         <div class="row no-gutters bg-purple-light justify-content-start pt-5 pb-5">
             <div class="col-6 pl-5 pr-5 mb-4">
                 <h2 class="title4-new text-white">Location</h2>
                 <h2 class="title2-new text-white">Location</h2>
                 <p class="text2-new text-white">Most applications of AI today are based on using data to train a machine learning model to pr</p>
             </div>
             <div class="col-12 pl-5 pr-5 mb-5">
                 <img src="/img/xai/app-similar%20data%20points.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>
         </div>

         <div class="row no-gutters bg-purple-light-alt justify-content-start pt-5 pb-5">
             <div class="col-6 pl-5 pr-5 mb-4">
                 <h2 class="title4-new text-white">Location</h2>
                 <h2 class="title2-new text-white">Location</h2>
                 <p class="text2-new text-white">Most applications of AI today are based on using data to train a machine learning model to pr</p>
             </div>

             <div class="col-12 pl-5 pr-5 mb-5">
                 <img src="/img/xai/app-list.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>
         </div>

         <div class="row no-gutters bg-purple-light justify-content-start pt-5 pb-5">
             <div class="col-6 pl-5 pr-5 mb-4">
                 <h2 class="title4-new text-white">Location</h2>
                 <h2 class="title2-new text-white">Location</h2>
                 <p class="text2-new text-white">Most applications of AI today are based on using data to train a machine learning model to pr</p>
             </div>
             <div class="col-12 pl-5 pr-5">
                 <img src="/img/xai/app-modal.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>

         </div>

         <div class="row no-gutters justify-content-center">
             <div class="col-12">
                 <img src="/img/xai/components.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>
         </div>

         <div class="row no-gutters bg-purple-light justify-content-start pt-5">
             <div class="col-6 pl-5 pr-5 mb-4">
                 <h2 class="title4-new text-white">Location</h2>
                 <h2 class="title2-new text-white">Color Studies</h2>
                 <p class="text2-new text-white">Most applications of AI today are based on using data to train a machine learning model to predict an outcome. While this interaction model works well where a task can be easily defined (automated), it fails to work when the task is not clearly defined. Music creation is one of many examples where the current blackbox-driven models don’t work, and existing state-of-the art algorithms are unable to perform well. For example, a musician looking to compose a new piece of music using existing machine. </p>
             </div>


             <div class="col-12">
                 <img src="/img/xai/color%20studies.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>


         </div>

         <div class="row no-gutters bg-purple-blue justify-content-start pt-5">
             <div class="col-6 pl-5 pr-5 mb-4">
                 <h2 class="title4-new">Location</h2>
                 <h2 class="title2-new">Color Studies</h2>
                 <p class="text2-new ">Most applications of AI today are based on using data to train a machine learning model to predict an outcome. While this interaction model works well where a task can be easily defined (automated), it fails to work when the task is not clearly defined. Music creation is one of many examples where the current blackbox-driven models don’t work, and existing state-of-the art algorithms are unable to perform well. For example, a musician looking to compose a new piece of music using existing machine. </p>
             </div>
             <div class="col-12">
                 <img src="/img/xai/blue%20color%20studies.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>
         </div>


     </div>



     <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
     <script>
         window.jQuery || document.write('<script src="/docs/4.3/assets/js/vendor/jquery-slim.min.js"><\/script>')
     </script>
     <script src="js/bootstrap.bundle.min.js" integrity="sha384-xrRywqdh3PHs8keKZN+8zzc5TX0GRTLCcmivcbNJWm2rs5C8PRhcEn3czEjhAO9o" crossorigin="anonymous"></script>
 </body>

 </html>