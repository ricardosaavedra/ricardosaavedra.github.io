 <!doctype html>
 <html lang="en">

 <head>
     <!-- Global site tag (gtag.js) - Google Analytics -->
     <script async src="https://www.googletagmanager.com/gtag/js?id=UA-145676546-1"></script>
     <script>
         window.dataLayer = window.dataLayer || [];

         function gtag() {
             dataLayer.push(arguments);
         }
         gtag('js', new Date());

         gtag('config', 'UA-145676546-1');
     </script>


     <meta charset="utf-8">
     <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
     <meta name="description" content="">

     <title>Gradient</title>

     <!-- Bootstrap core CSS -->
     <link href="css/bootstrap.min.css" rel="stylesheet" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">


     <!-- Custom styles for this template -->
     <link href="/css/style.css" rel="stylesheet">
 </head>

 <body>
     <!--Menu-->

     <nav class="navbar sticky-top bio pr-5 pl-5 no-gutters">
         <div class="col-12 col-md-4 text-md-left text-center">
             <a class="menu" href="index.html">Ricardo Saavedra</a>
         </div>

         <div class="col-12 col-md-4 text-md-center text-center">
             <a href="sol.html" class="menu">Prev</a>
             <span>/</span>
             <a href="NAM.html" class="menu">Next</a>
         </div>

         <!--Work/Info-->
         <div class="col-12 col-md-4 text-md-right text-center">
             <a href="index.html" class="menu-active menu">Work</a>
             <span>/</span>
             <a href="info.html" class="menu">Info</a>
         </div>
     </nav>

     <!--New top-->
     <div class="container-fluid full">
         <div class="row no-gutters">
             <div class="col-12 top-proj-container img-gradient">
                 <h1 class="proj-title2-new">A new approach to machine learning</h1>
                 <img src="/img/gradient/cover2.png" class="img-fluid gradient-img" alt="Responsive image">
             </div>
         </div>


     </div>

     <div class="container-fluid ">
         <div class="row bg-true-black mb-5">
             <div class="col-12 pl-5 pr-5 mb-5">
                 <h2 class="title3-new text-white">Gradient</h2>
                 <p class="text1-new text-white">Most applications of AI today are based on using data to train a machine learning model to predict an outcome. While this interaction model works well where a task can be easily defined (automated), it fails to work when the task is not clearly defined. Music creation is one of many examples where the current blackbox-driven models don’t work, and existing state-of-the art algorithms are unable to perform well. For example, a musician looking to compose a new piece of music using existing machine. <p>
             </div>


         </div>

         <div class="row">
             <div class="col-6 pl-5 pr-4">
                 <h2 class="title3-new">Approach</h2>
                 <p class="text2-new">Gradient is a self-initiated research project that combines user-centered machine learning design, data visualization, data science, and generative music to explore the question: "how can AI serve as a tool for cognitive augmentation”? In this work, I address this question by focusing on helping musicians in the composition process. I started the project by mapping out the space of AI research, evaluating available tools (apps, plugins, frameworks) their usability problems, and in general gathering insights about their designs. In addition, I worked with 6 composers throughout the design and ideation process to validate ideas and understand their needs.</p>

                 <h2 class="title3-new">Problem</h2>
                 <p class="text2-new">While machine learning works well in applications where a task is can be easily generalized form training data, it fails at augmenting users in tasks where the goal is diverse, personal and not clearly defined. For example, current tools fail at: delivering novelty (new combinations not seen in the data) and offering granular control to users. This in turn, makes it difficult for musicians to find desired musical outcomes, overwhelming them with an infinite number of ideas, all very unrelated to the artist’s initial intention. In summary, the current design paradigm that machine learning applications are based on needs to be challenged if AI is to be a tool that can augment human capabilities beyond what either a human or machine can achieve in isolation.</p>

                 <h2 class="title3-new">Acheivements</h2>
                 <p class="text2-new">Gradient introduces an alternative approach to designing with AI by focusing on making its algorithms transparent and easy to understand by its users. As a result, the interface allows composers to express their ideas by interacting with the algorithm to steer it towards novel outcomes. Instead of trying to create a system that composes for them, Gradient builds a way that musicians can compose with. By mixing simple components in a node-graph interface, new patterns can be reconfigured to create infinite new musical permutations. While Gradient focuses on music generation, the learnings acquired here can be used in different applications such as text generation or scientific discovery.</p>
             </div>

             <div class="col-3 pr-5 pl-4">
                 <h2 class="title3-new">Services</h2>
                 <p class="text2-new">Most applications of</p>

                 <h2 class="title3-new">Capabilities</h2>
                 <p class="text2-new">Most applications of</p>

                 <h2 class="title3-new">Approac</h2>
                 <p class="text2-new">Most applications of</p>


             </div>

             <div class="col-3 pr-5">
                 <h2 class="title3-new">Location</h2>
                 <p class="text2-new">Most applications of</p>

                 <h2 class="title3-new">Duration</h2>
                 <p class="text2-new">Most applications of</p>

                 <h2 class="title3-new">Methods</h2>
                 <p class="text2-new">Most applications of</p>

             </div>

         </div>
     </div>

     <div class="container-fluid ">
         <div class="row">
             <div class="col-12 pl-5 pr-5 border-top-bottom section-division ">
                 <h2 class="title1-new pr-5"><span>01</span>User Research</h2>
             </div>
         </div>

         <div class="row">

             <div class="col-6 pl-5 pr-4">
                 <h2 class="title3-new">Evaluating existing tools</h2>
                 <p class="text2-new">I began my research by evaluating existing tools by looking at their interaction models (how they work and what metaphors are they based on) and their information architecture. I set myself to use these tools to compose and explore musical ideas and see how they work. I followed a process similar to heuristic evaluation and task analysis, with the exception of not engaging a group of specialists. This step helped me understand how these applications differ, the spectrum of possibilities I could focus on, and what problems was I interested in solving moving forward.</p>

                 <p class="text2-new">My domain-specific knowledge in music composition and production allowed me to simulate the path a user might take from checking out websites, comparing value propositions, to installing and using tools.</p>
             </div>

             <div class="col-6 pl-4 pr-5">
                 <p class="text2-new">I was interested in mapping out:</p>
                 <ul class="bullet-new">
                     <li><span>Interaction models:</span> ahow is the system designed and how does the user interact with it? What are the options and outcomes?</li>
                     <li><span>Underlying technology:</span> what machine learning technologies are they using (deep learning vs GANs)?
                         Value proposition: how do they present themselves to artists? What features do they claim and what promises do they make?</li>
                     <li><span>Value proposition:</span> how do they present themselves to artists? What features do they claim and what promises do they make?</li>
                     <li><span>User base:</span> what user groups are they targeting?</li>
                     <li><span>Musical output:</span> what does the combination of interface and technology create? What kinds of musical output do certain systems prioritize?</li>

                 </ul>
             </div>
         </div>



         <div class="row mt-5 mb-5">
             <div class="col-6 pl-5 pr-4">
                 <h3 class="title2-new text-left">List of tools investigated</h3>
             </div>
             <div class="col-3 pl-4">
                 <ul class="list-new">
                     <li class="first-list">Magenta</li>
                     <li>Magenta Studio</li>
                     <li>Magenta Experiments</li>
                     <li>Endel</li>
                     <li>OpenAI Jukebox</li>
                     <li>OpenAI MuseNet</li>
                     <li>Orb Composer</li>
                 </ul>
             </div>

             <div class="col-3 pl-4 pr-5">
                 <ul class="list-new">
                     <li class="first-list">AIVA</li>
                     <li>Flow Machines</li>
                     <li>SuperCollider</li>
                     <li>Musico</li>
                     <li>Orca</li>
                     <li>Runway ML</li>
                     <li>Amper</li>
                 </ul>
             </div>


         </div>



         <div class="row justify-content-center">
             <div class="col-12 pl-5 pr-5 mb-5">
                 <h2 class="title2-new text-center">Key insights</h2>
             </div>

             <div class="col-6 pl-5 pr-4">
                 <ul class="list-new">
                     <li class="first-list"><span>Most tools are in the experimental stage:</span> these tools work well but are not necessarily useful for any professional use. They express ideas and show what’s possible.</li>
                     <li><span>Deep personalized music:</span> many projects in this category focus on generating music that tailor to user inputs (biofeedback, sensors, etc). This group of projects focuses on ambient music generation or custom-made tracks that match users’ intention for contexts such as meditation, workout or driving.</li>


                 </ul>
             </div>

             <div class="col-6 pl-4 pr-5">
                 <ul class="list-new">

                     <li class="first-list"><span>Licensing free AI-generated music:</span> the majority of the projects I investigated focus on videographers and vloggers as their user-base. The musical output isn’t anything unique, but that doesn’t defeat their purpose, which is generating easily accessible music at a low cost.</li>
                     <li><span>Frameworks for developers:</span> these tools make programming easier, but have a high learning curve and are often designed for creative coders. These frameworks feature certain model architectures and training algorithms.</li>

                 </ul>
             </div>

             <div class="col-10">
                 <img src="/img/gradient/personas.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>
         </div>

         <div class="row">
             <div class="col-6 pl-5 pr-4">
                 <h2 class="title3-new">Defining which users to focus on and why</h2>
                 <p class="text2-new">The evaluation enabled me to understand the whole gamut of users different tools were targeting, and how certain user groups are currently being served by the tools available. By defining what each group needs and the tasks they are trying to solve, I was able to see if the solutions available could be improved upon. Specifically, I was able to uncover that composers were the least served by existing tools. Their needs are very specific and are the most challenging to design for. Composers need tools that are offering fine-grained controls, are deeply customizable, while also being easy to use.. This in particular seems to be the focus in machine learning design over the next years: how do we make machine learning easily accessible not only to data scientists and programmers, but to everyone who wants to be augmented by what the technology brings?</p>

                 <p class="text2-new">Another major insight I had has to do with how the concept of augmentation was being used as a buzzword without careful consideration to what it might entail from an interaction design perspective. A lot of applications that claimed to augment composers in music creation tasks were in fact automating tasks for them.</p>


             </div>

             <div class="col-6 pl-4 pr-5">
                 <h2 class="title3-new">User Research Overview</h2>
                 <ol class="list-new">
                     <li>User interviews & contextual inquiry</li>
                     <li>Co-design exercises and sessions</li>
                     <li>Unfocused group panel discussion at CTM</li>
                 </ol>
             </div>
         </div>

         <div class="row">
             <div class="col-12 pl-5 pr-4 mb-5 mt-5">
                 <p class="title2-new">My interview process was focused mainly on qualitative understanding of their workflows, tasks and emotions regarding the creative process.</p>
             </div>
             <div class="col-6 pl-5 pr-4">
                 <h2 class="title3-new">User interviews</h2>
                 <p class="text2-new">Once I defined my user group, I set out to understand how different composers approach the task of music composition. I visited them at their studios instead of meeting them remotely to get a better sense of the tools and their environment. I was also interested in understanding how they thought about music conceptually, and how concepts were translated into how they approached their tools (software, hardware, instrument).</p>


             </div>
             <div class="col-6 pl-4 pr-5">
                 <p class="text2-new">I centered the interviews on a few open-ended questions to guide the discussion:</p>
                 <ul class="bullet-new">
                     <li><span>User flow:</span> How do they navigate from having an idea to finishing a composition? How do things begin and how things end?</li>
                     <li><span>Entry points:</span> What are the main entry points to their creative process? What are the structures that might guide the process?</li>
                     <li><span>Conceptual models:</span> What are the most prominent conceptual models that different composers have in common?</li>
                     <li><span>Previous experience with ML:</span> What are their frustrations, expectations and fascination with the technology?</li>
                 </ul>
             </div>

             <div class="col-12 pr-4 pl-5">
                 <img src="/img/gradient/quotes.png" class="img-fluid img-proj-new" alt="Responsive image">

             </div>
         </div>

         <div class="row justify-content-center mb-5 mt-5">
             <div class="col-12 pl-5 pr-5 mb-5">
                 <h2 class="title2-new text-center">Key insights</h2>
             </div>

             <div class="col-6 pl-5 pr-4">
                 <p class="title3-new">Entry points and axioms</p>
                 <ul class="list-new">
                     <li><span>Exploration vs. intention:</span> a tension between knowing what they want to hear and trying different things out until they find something</li>
                     <li><span>Errors & glitches:</span> exploring how tools brake as a form of finding interesting ideas and unexplored paths</li>
                     <li><span>Process-based:</span> musicians set up different processes ahead of time and plan their sessions before they engage in composing something</li>
                     <li><span>Copying other ideas as starting points:</span> using an existing idea to initiate the composition process, and eventually abandoning the original idea as the new idea takes shape</li>
                     <li><span>Seeds and prototypes:</span> ideas that can center the composition around one anchor point to inspire the development of new ideas</li>


                 </ul>
             </div>

             <div class="col-6 pl-4 pr-5">
                 <p class="title3-new">Conceptual models: <span>How do they think about musical events</span></p>
                 <ul class="list-new">
                     <li><span>Time-based notation:</span> notes on a page followed by their intrinsic relationship (composers and technically trained musicians)</li>
                     <li><span>Frequency spectrum:</span> instruments are spread across frequency spectrum (low to high)</li>
                     <li><span>Gesture-based:</span> instruments and notes are connected by chain of events and are bound together (Improvisation ensembles, complex chain of events)</li>
                     <li><span>Networks:</span> musical events are the result of interaction between agents in a network (modular synth, Max/MSP)</li>
                     <li><span>Agents, stage & storytelling:</span> each agent is a sound that interacts with other agents to tell the story, entering the stage at different times to express different ideas</li>
                     <li><span>Rhythm-centric:</span> a rhythmic idea that centers the music piece (techno, percussive tracks)</li>
                     <li><span>Harmony and melody:</span> melody moves on top of a narrative in different rhythms (counterpoint)</li>
                 </ul>
             </div>
         </div>

         <div class="row bg-gray-black justify-content-start pt-5 pb-5">
             <div class="col-6 pr-4 pl-5 mb-4">
                 <h2 class="title4-new text-white">Unfocus group CTM</h2>
                 <p class="text2-new text-white">In addition to the user interview, I also led a panel discussion with 5 different musicians about their experience in working with machine learning in their creative process. The recording of the panel can be seen in full length here: </p>
             </div>


             <div class="col-12 pl-5 text-center mb-5">
                 <iframe width="70%" height="600" src="https://www.youtube.com/embed/l90W2mOEn-k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

             </div>
         </div>

         <div class="row justify-content-start mt-5 pt-5">
             <div class="col-12 pr-4 pl-5 mb-5">
                 <h2 class="title2-new text-center ">Summarizing insights</h2>
                 <p class="title4-new text-center ">User problems with pre-trained models in artistic context
                 </p>
             </div>

             <div class="col-6 pl-5 pr-4 text-left">
                 <ul class="list-new">
                     <li><span>Expectation vs. reality</span> <br>All artists seem eager to try out Ml in the creative process, but get frustrated after trying existing tools</li>
                     <li><span>Different backgrounds</span> <br>Music education isn’t equal amongst artists, and not all artists are knowledgeable about music theory</li>

                     <li><span>Augmentation instead of automation</span> <br>All artists were opposed to having the model generating fully fledged compositions</li>
                 </ul>
             </div>

             <div class="col-6 pl-5 pr-4 text-left">
                 <ul class="list-new">
                     <li><span>Problems with curation</span> <br>All users complained about inability to sort through the model output since the possibilities are endless</li>

                     <li><span>Lack of originality</span> <br>Musicians complained about not being able to get unique results from apps that used generic training data</li>

                     <li><span>Lack of transparency</span> <br>There’s not a way for artists to communicate with the model, which makes for a very frustrating creative experience</li>
                 </ul>
             </div>
         </div>



     </div>

     <div class="container-fluid ">
         <div class="row">
             <div class="col-12 pl-5 pr-5 border-top-bottom section-division ">
                 <h2 class="title1-new pr-5"><span>02</span>Problem Definition</h2>
             </div>
         </div>


         <div class="row justify-content- pb-5">
             <div class="col-6 pr-4 pl-5 mb-5">
                 <h2 class="title2-new ">How might we? </h2>
                 
             </div>

             <div class="col-6 pl-5 text-left mb-5">
                 <ul class="bullet-new">
                     <li><span>Augment</span> artists in their creative process with a high level of granularity?</li>
                     <li><span>Design</span> interfaces that mediate the communication with the algorithm to enhance artists’ workflow?</li>
                     <li><span>Create</span> ways that people can shape and train their algorithms to their own needs through transparent interfaces?</li>
                     <li><span>Shift away</span> from the random output model to something you can actually understand and manage?</li>
                     <li><span>Give</span> artists full control of the algorithms without having to code their models and become programmers?</li>
                     <li><span>Design</span> an expressive language that is natural and intuitive and doesn’t exclude self-taught musicians?</li>
                 </ul>
             </div>

             <div class="col-12 pr-4 pl-5">
                 <p class="title4-new text-left">Shallow learning: a new approach to machine learning design </p>
                 <h2 class="title2-new">If deep learning is defined by large datasets, narrow tasks, and large computing power, “shallow learning” is the exact opposite: simple models that need human guidance to work, are transparent, run well locally, and work well with small datasets.</h2>
             </div>
         </div>

         <div class="row justify-content- pt-5 pb-5">
             <div class="col-6 pr-4 pl-5 mb-5">
                 <h2 class="title3-new ">Design principles</h2>
                 <p class="text2-new">Design principles are a good way to define the principles that the design solutions should capture. They work as safeguards for quickly validating the different ideas that might come up along the way. Good design principles describe the most important elements of the solution without being prescriptive and work to align future solutions with themes found earlier in the ideation phase.</p>

             </div>

             <div class="col-6 pl-5 pr-5 text-left">
                 <ol class="list-new">
                     <li><span>Example-based interaction:</span> musical examples are in itself a way to communicate intent with the model</li>
                     <li><span>Simple vs complex workflows:</span> the experience offers simple modes with advanced controls when necessary</li>
                     <li><span>Open-ended and diverse:</span> the language structure can be further refined by users themselves through learning </li>
                     <li><span>It grows with users:</span> knowledge accumulates, and the system is biased towards user concepts and patterns learned from previous interactions</li>
                     <li><span>Errors don’t exist:</span> regardless of user input, the system should also produce sounds and musical events </li>
                     <li>Expressive, yet abstract interfaces: balance between granular control and abstract forms of expression
                     </li>

                 </ol>
             </div>
         </div>
     </div>

     <div class="container-fluid">
         <div class="row">
             <div class="col-12 pl-5 pr-5 border-top-bottom section-division ">
                 <h2 class="title1-new pr-5"><span>03</span>Ideation & Solution</h2>
             </div>
         </div>

         <div class="row  justify-content-center">
             <div class="col-8 mb-6">
                 <img src="/img/gradient/mml-opening.png" class="img-fluid img-proj-new" alt="Responsive image">
             </div>
             <div class="col-6 pr-4 pl-5">
                 <h2 class="title3-new ">Music Machine Language: a no-code visual programming language</h2>
                 <p class="text2-new">MML is a visual language that enables the model to express the concepts it learned from training data visually. By exposing the model toa visual interface, users can further manipulate the output by changing the weights, operators, connections, and parameters. MML works in two ways: it tells users what the model sees, and offers musicians a way to steer the algorithm towards novel outcomes the model has never seen. Instead of exposing the neural network as it is to users, Gradient abstracts the parameters through simpler components to achieve the best balance between granularity, usability and control. </p>
             </div>

             <div class="col-6 pl-4 pr-5 text-left">
                 <ol class="list-new">
                     <li>Visual abstraction to mediate how musicians think about musical ideas vs. how the algorithm understands music</li>
                     <li>Works in two ways: you can use the language to visualize musical concept, as well as use it to express new ideas</li>
                     <li>You can program musical relationships without having to code them by hand</li>
                 </ol>
             </div>
         </div>

         <div class="row mb-6">
             <div class="col-12 pr-4 pl-5 mb-5">
                 <img src="/img/gradient/scores.png" class="img-fluid img-proj-new" alt="Responsive image">

             </div>
             <div class="col-6 pr-4 pl-5 mb-5">
                 <h2 class="title3-new ">Abstract scores and non-conventional music notation</h2>
                 <p class="text2-new">MML draws heavily on the abstract musical scores experiments done in the 60s and 70s by electronic musicians and avant-garde composers. In these scores, artists explored ways to encode musical ideas through visual abstractions as opposed to composing notes on a page (music notation). I was interested in exploring ideas that lived in-between music notation and abstract visual representation, but that could also be used to generate ideas instead of only representing them visually (two-way connection). I spent a month trying to find a solution that achieves such balance, and that could be robust enough to encode a variety of ideas. Once I found a potential way to solve the problem, I set out to test the solution implementing a proof of concept in Max/MSP. The proof of concept featured very few operators, but was able to work seamlessly as a no-code programming language to assist the composition process.</p>
             </div>

             <div class="col-6 pl-4 pr-5 text-left">
                 <img src="/img/gradient/iannis.png" class="img-fluid img-proj-new" alt="Responsive image">
                 <figcaption class="figure-caption">UPIC, a system designed by Iannis Xenakis, translates graphic notation to sound on a 1:1 relationship.</figcaption>
             </div>
         </div>

         <div class="row bg-true-black justify-content-start pt-5 mt-5">
             <div class="col-12 pr-5 pl-5 mb-4">
                 <h2 class="title2-new text-white text-center">MML: a no-code visual programming language</h2>

             </div>

             <div class="row no-gutters justify-content-center mb-5">
                 <div class="col-12">
                     <img src="/img/gradient/explaining-mml.png" class="img-fluid img-proj-new" alt="Responsive image">
                 </div>
             </div>
         </div>

         <div class="row mt-5">
             <div class="col-12 pr-4 pl-5 mt-5 mb-5">
                 <h2 class="title2-new text-center ">Defining    the flows</h2>
             </div>
             <div class="col-12 pr-4 pl-5 mb-5">
                 <img src="/img/gradient/flows.png" class="img-fluid img-proj-new" alt="Responsive image">

             </div>

             
             <div class="col- pr-4 pl-5 mb-5 mt-5">
                 <h2 class="title2-new ">I designed the app to match musicians’ approach to the composition process, and as such it has four different entry points: </h2>
             </div>
             <div class="col-6 pr-4 pl-5 mb-5">
                 <img src="/img/gradient/logo.png" class="img-fluid img-proj-new" alt="Responsive image">

             </div>

             <div class="col-6 pl-5 pr-4 text-left">
                 <ol class="list-new">
                     <li><span>Import midi seeds into Gradient</span> to start a new composition process: musicians need to first import midi files. MIDI is a protocol to encode musical events, similar to music notation. MIDI files can have one to many instruments on a timeline.</li>
                     <li><span>Manipulate seeds:</span> once the musical ideas (seeds) have been imported and analyzed by the algorithm*, users can change the visual node-graph generated by the algorithm to change the musical output. By changing the visual graph, it’s possible to compose new pieces of music, or generate different variations of existing ideas</li>
                     <li><span>Combine different seeds in different ways:</span> multiple seeds (musical ideas) can be imported into Gradient. Once imported, they can be mixed in different ways. Each seed features an input and output which enables composers to mix these ideas in infinite ways and one idea can be used to modulate different ideas to generate new outputs. For instance, you can mix a drum pattern with a piano to have them playing closely together.</li>
                     <li><span>Record, and finish ideas</span> gradient works well with existing music software, so that the output (music events) can be recorded into softwares such as Ableton, proTools or logic very easily. Gradient isn’t designed to compose fully fledged ideas, therefore, having a chance to let musicians edit and finish compositions is an essential part of the workflow. </li>
                 </ol>
             </div>

             <div class="col-6 pl-4 pr-5 text-left">
                 <ul class="list-new">
                     
                 </ul>
             </div>
         </div>

         <div class="row bg-true-black justify-content-start pt-5">
             <div class="col-12 pr-5 pl-5 mb-4">
                 <h2 class="title2-new text-white text-center">Building a high-fidelity prototype</h2>

             </div>

             <div class="row no-gutters justify-content-center mb-5">
                 <div class="col-12">
                     <img src="/img/gradient/prototype.png" class="img-fluid img-proj-new" alt="Responsive image">
                 </div>
             </div>
         </div>

         <div class="row bg-gray-black justify-content-start pt-5 pb-5">
             <div class="col-6 pr-5 pl-5 mb-5">
                 <h2 class="title4-new text-white">Importing MIDI files</h2>
                 <p class="text2-new text-white">Once a midi file is imported into Gradient, the software builds a map of the relationship between note events, and how instruments are related to each other. The system looks into all the instruments at once, and tries to find groups and relationships between them. For instance, two instruments could be linked together or in opposition to each other, either by events (one only happens when the other is mute) or by interval and harmonic relationships. Once analyzed, these relationships are plotted onto the visual language.</p>
             </div>

             <div class="row no-gutters justify-content-center mb-5">
                 <div class="col-10">
                     <!-- The video -->
                     <video width="100%" autoplay muted loop controls>
                         <source src="img/gradient/importing%20midi.mp4" type="video/mp4">
                     </video>
                 </div>
             </div>
         </div>

         <div class="row bg-true-black justify-content-start pt-5 pb-5">
             <div class="col-12 pr-5 pl-5 mb-5">
                 <h2 class="title4-new text-white">Deleting nodes and changing connections</h2>
             </div>

             <div class="row no-gutters justify-content-center mb-5">
                 <div class="col-10">
                     <!-- The video -->
                     <video width="100%" autoplay muted loop controls>
                         <source src="img/gradient/deleting%20new%20nodes.mp4" type="video/mp4">
                     </video>
                 </div>
             </div>
         </div>

         <div class="row bg-gray-black justify-content-start pt-5 pb-5">
             <div class="col-12 pr-5 pl-5 mb-5">
                 <h2 class="title4-new text-white">Using MML to compose new ideas</h2>
             </div>

             <div class="row no-gutters justify-content-center mb-5">
                 <div class="col-10">
                     <!-- The video -->
                     <video width="100%" autoplay muted loop controls>
                         <source src="img/gradient/inspecting%20probabilities%20seed3.mp4" type="video/mp4">
                     </video>
                 </div>
             </div>
         </div>

         <div class="row bg-true-black justify-content-start pt-5 pb-5">
             <div class="col-12 pr-5 pl-5 mb-5">
                 <h2 class="title4-new text-white">Connecting seeds to create new ideas</h2>
             </div>

             <div class="row no-gutters justify-content-center mb-5">
                 <div class="col-10">
                     <!-- The video -->
                     <video width="100%" autoplay muted loop controls>
                         <source src="img/gradient/connectting%20nodes%20main%20nodes.mp4" type="video/mp4">
                     </video>
                 </div>
             </div>
         </div>

     </div>



     <div class="container-fluid ">
         <div class="row">
             <div class="col-12 pl-5 pr-5 border-top-bottom section-division no-margin-top">
                 <h2 class="title1-new pr-5"><span>04</span>Conclusion</h2>
             </div>
         </div>

         <div class="row mb-5">
             <div class="col-6 pl-5 pr-5">
                 <h2 class="title3-new">Validation and next steps</h2>
                 <p class="text2-new">I was able to validate the main concepts regarding the interface design, user flows, and managed to build a working prototype in Max/Msp to validate the technical viability of the system. I’m currently looking for research partners, funding, and ML engineers to join the research exploration, and to continue developing the project into a working prototype to further iterate on the most challenging aspects of the project. Please get in touch if you’d like to hear more about the project, or if your interests are aligned with the research outlined here.</p>
             </div>
         </div>
     </div>

     <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
     <script>
         window.jQuery || document.write('<script src="/docs/4.3/assets/js/vendor/jquery-slim.min.js"><\/script>')
     </script>
     <script src="js/bootstrap.bundle.min.js" integrity="sha384-xrRywqdh3PHs8keKZN+8zzc5TX0GRTLCcmivcbNJWm2rs5C8PRhcEn3czEjhAO9o" crossorigin="anonymous"></script>
 </body>

 </html>