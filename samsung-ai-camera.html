<!doctype html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-145676546-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-145676546-1');
    </script>


    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">

    <title>Samsung AI Camea</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">


    <!-- Custom styles for this template -->
    <link href="/css/style.css" rel="stylesheet">
</head>

<body>
    <!--Menu-->

    <nav class="navbar sticky-top bio no-gutters">
        <div class="col-12 col-md-4 text-md-left text-center">
            <a class="menu" href="index.html">Ricardo Saavedra</a>
        </div>

        <div class="col-12 col-md-4 text-md-center text-center">
            <a href="optik.html" class="menu">Prev</a>
            <span>/</span>
            <a href="gearS.html" class="menu">Next</a>
        </div>

        <!--Work/Info-->
        <div class="col-12 col-md-4 text-md-right text-center">
            <a href="index.html" class="menu-active menu">Work</a>
            <span>/</span>
            <a href="info.html" class="menu">Info</a>
        </div>
    </nav>

    <div class="container-fluid full">
        <!--main image-->
        <div class="row mt-4 justify-content-center no-gutters">
            <div class="col-12 col-md-9 project-title">
                <h1 class="display-title2 text-center">Samsung AI Camera</h1>
                <h1 class="display-title1 text-center">Computer vision and machine learning to enable new interaction models: autonomous capture instead of intentional creation</h1>
            </div>
            <div class="col-12 col-md-7">
                <p class="text1">Ikon is a wearable concept designed around computer vision and machine learning to enable new autonomous interaction models. The project builds on concepts explored in life-logging experiments and task automation. The system uses a combination of algorithms to determine an appropriate moment to trigger a recording. After multiple clips are taken, they are autonomously processed and stitched together to create short films. Google released <a href="https://www.theverge.com/2018/2/27/17055618/google-clips-smart-camera-review">(Google Clip)</a> a very similar camera 3 years after the concept was developed. Ikon is one of the many explorations that emerged from year-long research on the future of image capturing devices, taking place between 2013 and 2014—involving at times more than 20 designers, software and hardware engineers. </p>
            </div>

            <div class="col-12 col-md-3 offset-md">
                <p class="text3">
                    <span>Discipline:</span> Wearable, product & hardware development<br>
                    <span>Role:</span> Lead designer, user research, software design<br>
                    <span>Team:</span> 10-20 designers, software developers and engineers<br>
                    <span>Year:</span> 2014<br>
                </p>
            </div>
        </div>


        <div class="row no-gutters">
            <div class="col-12 mb-5">
                <div class="img-proj">
                    <img src="/img/ikon/ikon-cover.jpg" class="img-fluid" alt="Responsive image">
                </div>
            </div>
        </div>

        <!--main image-->
        <div class="row no-gutters">
            <div class="col-md-6 mb-4 offset-md-1">
                <p class="display-title3">Concept and research</p>
                <p class="text2">Ikon explored the concept of automation instead of augmentation; autonomous capture instead of intentional creation. After conducting many user interviews, it became clear that video editing was a task most users were not interested in performing. Furthermore, our research showed that most users never re-watched videos taken at a given event. The reasons ranged from the often long clip's duration to the lack of continuity in reviewing them (clips needed to be previewed one-by-one). Consequentially, recordings were rarely edited to compose short films.</p>

                <p class="text2">The proposed UX model positioned users as curators instead of creators. The software encoded editing heuristic in the form of “recipes" that were used to process raw content. Borrowing from computational art practices, chance operations (John Cage), glitch, and procedural algorithms, the system was designed to create interesting accidents. In other words, even if the algorithm failed to output visually pleasing results, users still enjoyed watching generated films that contained moments they wouldn’t have taken otherwise. Conversely, the experience revolved around passive consumption: users kept only the edits they found interesting. While still requiring a form of user engagement, the model proved to be less demanding than conventional editing workflows.</p>

                <p class="display-title3">Main challenges</p>
                <p class="text2">During the prototype phase, challenges involved circumventing the camera's output quality—videos were often blurry, and rarely featured a conventional film framing)—and the short battery life that resulted from intensive file transmission. Computer vision engineers circumvented the problem by developing custom algorithms that parsed out the clips that didn’t meet basic technical standards. Moreover, the engineering team managed to develop low-consumption protocols that extended battery life considerably.</p>

                <p class="display-title3">My role</p>
                <p class="text2">During the year-long research phase, I led many efforts ranging from coordinating user research and technical investigations to designing and developing many camera-related concepts, features, IPs, and UI/UX innovations. During the development phase, my role consisted of making the original vision a reality. During this stage, I collaborated with interaction and industrial designers, engineers and computer vision specialists to work through many technical challenges. More specifically, the work consisted of experimenting with video generation algorithms and computer vision classification methods in the form of iterative prototyping, eventually leading to a presentable working demo.</p>


            </div>
        </div>

        <div class="row no-gutters justify-content-center">

            <div class="col-11">
                <img src="/img/ikon/ikon-white.jpg" class="img-fluid img-proj" alt="Responsive image">

            </div>

            <div class="col-11 mb-5 mt-4">
                <img src="/img/ikon/colla2.jpg" class="img-fluid img-proj" alt="Responsive image">

            </div>

        </div>

        <div class="w-100"></div>
        <div class="col-12">
        </div>

    </div>


    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script>
        window.jQuery || document.write('<script src="/docs/4.3/assets/js/vendor/jquery-slim.min.js"><\/script>')
    </script>
    <script src="js/bootstrap.bundle.min.js" integrity="sha384-xrRywqdh3PHs8keKZN+8zzc5TX0GRTLCcmivcbNJWm2rs5C8PRhcEn3czEjhAO9o" crossorigin="anonymous"></script>
</body>

</html>